# CommonsAI: A Reusable-Answers Private LLM Community

**Product Requirements Document (PRD) v0.1**

---

> **Mission**: When User B encounters a problem similar to User A's, CommonsAI should **first hit a validated, reusable answer (Recipe) and/or the original problem trajectory (Trajectory)**—cutting repeated generation, reducing cost and latency, and improving one-shot success and satisfaction.

---

## Table of Contents

- [1. Background \& Vision](#1-background--vision)
- [2. Goals \& Non-Goals](#2-goals--non-goals)
- [3. Vertical Focus (Cold-Start)](#3-vertical-focus-cold-start)
- [4. User Personas \& Key Journeys](#4-user-personas--key-journeys)
- [5. Core Concepts \& Data Model](#5-core-concepts--data-model)
- [6. Functional Requirements (MVP)](#6-functional-requirements-mvp)
- [7. Non-Functional Requirements](#7-non-functional-requirements)
- [8. Milestones \& Acceptance](#8-milestones--acceptance)
- [9. Metrics](#9-metrics)
- [10. Open-Core Strategy](#10-open-core-strategy)
- [11. Risks \& Mitigations](#11-risks--mitigations)
- [12. Appendix](#12-appendix)

---

## 1. Background & Vision

Not every question needs to be regenerated by an LLM. For public/academic/common issues, a single high-quality answer should be **searchable and reusable**.

CommonsAI offers a **searchable, reusable Q&A community**:

- Search existing **Trajectories + final accepted Recipes** first
- On a confident hit, return an **Instant Answer**
- If no hit, switch to LLM-assisted resolution and, once accepted by the user, persist the result to the knowledge base—forming a **data flywheel**

Current MVP includes cross-modal retrieval (e.g., OpenCLIP), local indexing (e.g., FAISS), and baseline APIs (`/query`, `/update_answer`, `/add_image`, `/set_tau`).

---

## 2. Goals & Non-Goals

### 2.1 Business Goals (MVP → Beta)

1. **Cost/Latency Savings**: vs. "generate-only" baseline, reduce **token cost by ≥40%** per session and **p50 latency by ≥30%**
2. **Hitting Reusable Answers**: after Top-K retrieval + two-stage rerank, achieve **Hit@3 ≥ 35%** within the chosen vertical
3. **Quality & Trust**: **7-day reuse rate ≥ 20%** for accepted answers; CSAT ≥ 4.2/5
4. **Privacy by Default**: public corpus stores only **redacted Recipes**; users can revoke at any time

### 2.2 Non-Goals (for MVP)

- No broad domain coverage; focus on **one vertical** (see §3)
- No heavy social features (follow/feeds/leaderboards, etc.)
- No heavy-duty trust computing; rely on **basic rules + human review queue**

---

## 3. Vertical Focus (Cold-Start)

**Recommended**: **Developer error troubleshooting** (text/screenshots/code)  
**Alternatives**: academic tools, Notion/Obsidian workflows

**Selection heuristics**: high repeatability, text+image modality, measurable ROI (time/money saved), clear willingness to pay.

---

## 4. User Personas & Key Journeys

### 4.1 Personas

- **Askers (A/B)**: engineers/grad students/knowledge workers who submit text or screenshot queries; expect a **validated final fix** quickly
- **Contributors**: refine and enrich Recipes with steps/pitfalls; may opt-in to share "generic parts"
- **Moderators/Admins**: handle violations, merge duplicates, maintain tags and versions

### 4.2 Journeys

1. **User A** asks (text/screenshot) → Top-K retrieval → on high confidence, return **Instant Answer (Recipe)**; otherwise switch to LLM → once accepted, persist
2. **User B** later asks a similar question → hits the validated **Trajectory/Recipe** → resolves in one hop
3. **Contributors** submit incremental improvements → version history → better hit-rate and quality

---

## 5. Core Concepts & Data Model

### 5.1 Concepts

- **Trajectory**: the full path from **first query → several iterations → final accepted answer**
- **Recipe**: a **normalized, reusable solution card** distilled from multiple similar Trajectories (steps, preconditions, params, pitfalls, applicable versions, evidence)

### 5.2 Schemas (v0.1 Draft)

#### Trajectory

```json
{
  "id": "traj_...",
  "created_at": "2025-08-31T00:00:00Z",
  "user_id": "u_...",
  "modality": ["text", "image"],
  "query_text": "...",
  "image_refs": ["s3://.../shot1.png"],
  "ocr_text": "...",
  "context": {"code": "...", "env": "python3.11", "os": "macOS"},
  "messages": [{"role": "user/assistant", "content": "..."}],
  "final_answer": "markdown ...",
  "quality": {"accepted": true, "score": 0.92},
  "tags": ["python", "faiss", "ocr"],
  "embeddings": {"text": [..], "image": [..], "ocr": [..]},
  "fingerprints": {"simhash": "...", "minhash": "..."}
}
```

#### Recipe

```json
{
  "id": "rcp_...",
  "title": "Standard fix for ImportError: xxx",
  "summary": "One-liner problem + scope",
  "preconditions": ["python >= 3.10", "macOS ARM64"],
  "steps": ["1. Check ...", "2. Run ...", "3. Restart ..."],
  "params": {"ENV_VAR": "...", "FLAG": "..."},
  "pitfalls": ["Version notes ...", "Permissions ..."],
  "evidence": {"trajectory_ids": ["traj_a", "traj_b"], "screenshots": ["..."]},
  "versions": [{"v": "1.0", "date": "2025-08-31", "changes": "..."}],
  "metrics": {"reuse_count": 12, "last_7d_success": 0.71}
}
```

---

## 6. Functional Requirements (MVP)

### 6.1 Retrieval & Indexing

**Multi-modal embeddings**:
- **Text**: general-purpose embeddings (e.g., BGE-M3)
- **Image**: OpenCLIP/SigLIP vectors
- **OCR**: extract text via Tesseract/PaddleOCR and embed as text

**Hybrid retrieval**: run BM25 (lexical), text vectors, image vectors, and OCR-text vectors in parallel; fuse with **RRF (Reciprocal Rank Fusion)**; send Top-K through a **cross-encoder reranker**.

**Vector store**: Qdrant (multi-vector/hybrid-friendly) or pgvector (co-located with business tables). MVP can start with local FAISS, then migrate online.

### 6.2 Confidence Threshold & Instant Answer

Maintain a threshold `tau`: if fused score ≥ `tau`, show **Instant Answer Card** (Recipe + confidence + applicable versions + verification count).

Low confidence → switch to LLM generation; present candidate Recipes to the user; once **accepted**, write back and update clustering.

### 6.3 Trajectory Consolidation

Persist **accepted** sessions as Trajectories; merge/cluster similar ones to upgrade or create **Recipes**.

### 6.4 Ingestion & Upload

**Minimal inputs**:
- Text paste / screenshot upload / error logs
- Bulk import: `qa.jsonl` / image folder (for initial seeding)

### 6.5 Redaction & Sharing

Private by default; public corpus stores only **redacted Recipes** (auto rules + manual editing); revocable; auditable.

### 6.6 Violations & Moderation (Basic)

Keyword/rule triggers → **human review queue**.  
Basic copyright guidelines (no full paid content dumps, etc.).

### 6.7 Cost/Latency Panel

On the result page, display **token & time saved** estimates:
- `token_saving = gen_cost_baseline − reuse_cost`
- `time_saving = latency_baseline − latency_reuse`

### 6.8 APIs (extend current set)

**Current**: `/health`, `/query`, `/update_answer`, `/add_image`, `/set_tau`

**Proposed additions**:
- `/ingest_text`, `/ingest_image` (OCR toggle)
- `/search_hybrid` (per-channel scores + fused RRF + Top-K)
- `/rerank` (stage-2)
- `/instant_answer` (threshold→Recipe card)
- `/share_recipe` (redact & publish)
- `/moderation/queue` (review workflow)

### 6.9 Frontend (MVP)

- **Search**: input (text/image/paste screenshot) → results (Instant Answer Card / similar threads / "start new" CTA)
- **Detail**: Trajectory timeline + Recipe versions; right-side actions (Accept / Improve / Share)
- **Upload**: bulk import; redaction assistant; preview

---

## 7. Non-Functional Requirements

- **Latency**: p50 < **1.2s** (Instant Answer); p50 < **3s** (LLM path)
- **Cost**: vs. generation-only, **≥40% token** savings per 1k queries; online inference budget ≤ **X** per 1k queries (model/region dependent)
- **Observability**: per-channel hit-rate, RRF weight contribution, rerank lift, threshold hit-rate, fallback rate, reuse rate, revoke rate
- **Security & Privacy**: private by default; audit & revoke; redacted public corpus; minimal viable moderation

---

## 8. Milestones & Acceptance

*Mapped to the "10 immediate steps"*

### M0 (Weeks 1–2)

1. Pick the vertical (Dev error troubleshooting)
2. Design **Trajectory/Recipe** schemas and multi-modal fingerprints
3. Wire up **text/image/OCR** embeddings and storage
4. Implement **RRF fusion** (BM25 + text + image + OCR)

**Acceptance**: offline Hit@10 ≥ **50%**; fused ranking returned.

### M1 (Weeks 3–4)

5. Add **threshold → Instant Answer**; apply **cross-encoder reranker** on Top-K
6. Release minimal upload (text/image/error paste)

**Acceptance**: online A/B shows p50 down **≥20%**; Instant Answer CTR **≥30%**.

### M2 (Weeks 5–6)

7. **One-click redaction & sharing** flow
8. Show **token/time saved** on results page

**Acceptance**: public corpus **≥100 Recipes**; visible saving metrics.

### M3 (Weeks 7–8)

9. **Fallback**: below-threshold → LLM; on acceptance, write-back
10. **Moderation**: basic violation rules + human queue

**Acceptance**: closed fallback loop; moderation SLA < **24h**.

---

## 9. Metrics

### Offline
- Recall@K / MRR / NDCG
- Cluster dedup rate
- OCR recall contribution
- Image-vector recall contribution
- Rerank lift Δ

### Online
- Hit@K (Instant Answer rate)
- One-shot success
- Token/latency savings
- Reuse rate
- 2nd-hop rate
- CSAT
- Revoke rate
- Violation rate

---

## 10. Open-Core Strategy

### Open
- Schemas/protocols
- Retrieval adapters (RRF/rerank interfaces)
- Client SDK
- Minimal frontend

### Closed
- High-quality public corpus data
- Evaluation baselines
- Moderation/abuse tooling
- SaaS ops

### Defensive Publication
Whitepaper describing approach and metrics—without critical implementation or data.

---

## 11. Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| **Low/false hits** | Expand training set; add cross-encoder; bucketed & dynamic thresholds |
| **Modality gaps** | OCR variance → multi-engine and post-processing |
| **Compliance** | Private-by-default, strong redaction, revocation |
| **Cold start** | Single vertical + bulk ingest + contribution prompts |
| **Cost overrun** | Reuse priority; cheapest viable LLM path; caching |

---

## 12. Appendix

### 12.1 Retrieval Fusion (RRF + Stage-2) Pseudocode

```python
# ranks_* are 1-based ranks from each channel; k is RRF constant (typical 60)
score = 0.0
for r in [rank_lexical, rank_text, rank_image, rank_ocr]:
    if r is not None:
        score += 1.0 / (k + r)
# Take Top-K by fused score, then apply a cross-encoder reranker
```

### 12.2 Threshold & Fallback

- If `score >= tau` → show **Instant Answer Card**; else go LLM
- On acceptance → persist as Trajectory; if a similar Recipe cluster exists → merge; else create new

### 12.3 Observability Fields (Examples)

`query_id, hit_at_k, rr, ndcg, cost_baseline, cost_actual, saved_tokens, saved_ms, used_channels, tau, rerank_gain, accepted`

---

**Version**: v0.1 (2025-08-31)  
**Owner**: You  
**Collaborators**: TBA  
**Milestones**: M0–M3 (8 weeks)

---

## Contributing

We welcome contributions! Please see our [contributing guidelines](CONTRIBUTING.md) for details.

## License

This project is licensed under the [MIT License](LICENSE).
